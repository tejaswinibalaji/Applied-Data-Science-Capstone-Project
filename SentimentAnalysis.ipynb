{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Download IMDB Data \nThis note book is to download and save imdb data "}, {"metadata": {}, "cell_type": "code", "source": "import sys\nimport os\nimport urllib.request\nimport tarfile\nimport zipfile\n\nimport os\nimport glob\n\nimport nltk\nnltk.download('stopwords')\n\n", "execution_count": 75, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/dsxuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n", "name": "stderr"}, {"output_type": "execute_result", "execution_count": 75, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import requests\nfrom pathlib import Path", "execution_count": 44, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\nr = requests.get(url, allow_redirects=True)", "execution_count": 21, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "open('aclImdb_v1.tar.gz', 'wb').write(r.content)", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "84125825"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "t = tarfile.open('aclImdb_v1.tar.gz', 'r').extractall('.') ", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "\u001b[0m\u001b[01;34maclImdb\u001b[0m/  aclImdb_v1.tar.gz  imdb\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "def _read_text_file(path):\n\n    with open(path, 'rt',encoding=\"utf8\") as file:\n        \n        lines = file.readlines()\n\n        text = \" \".join(lines)\n\n    return text\n", "execution_count": 42, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pathlist_pos = Path('/home/dsxuser/work/aclImdb/train/pos').rglob('*.txt')\ndata_pos = [_read_text_file(path) for path in pathlist_pos]\npathlist_neg = Path('/home/dsxuser/work/aclImdb/train/neg').rglob('*.txt')\ndata_neg=[_read_text_file(path) for path in pathlist_neg]", "execution_count": 52, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "x_train_text = data_pos + data_neg\ny_train = [1.0] * len(data_pos) + [0.0] * len(data_neg)", "execution_count": 53, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pathlist_pos_test = Path('/home/dsxuser/work/aclImdb/test/pos').rglob('*.txt')\ndata_pos_test = [_read_text_file(path) for path in pathlist_pos_test]\npathlist_neg_test = Path('/home/dsxuser/work/aclImdb/test/neg').rglob('*.txt')\ndata_neg_test=[_read_text_file(path) for path in pathlist_neg_test]", "execution_count": 54, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "x_test_text = data_pos_test + data_neg_test\ny_test = [1.0] * len(data_pos_test) + [0.0] * len(data_neg_test)", "execution_count": 55, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "x_train_text[1]", "execution_count": 73, "outputs": [{"output_type": "execute_result", "execution_count": 73, "data": {"text/plain": "'I voted 8 for this movie because of some minor childish flaws. Other than that, this movie is one of my favorites! It\\'s entertaining to say the least. The shooting scenes are ridiculous though, and I think Gackt (who wrote the book) takes a little bit too much of his \"Matrix obsession\" into it. It seems like their enemies just stands there...waiting to get shot at. However, this movie is touching and it always makes me cry. It has a lot of GREAT humor in it so it makes me laugh as well. Gackt is a superb actor I must say..he shows so much emotion. This was Hyde\\'s first time acting and he did okay. The role fits him. Wang Lee Hom is absolutely great. The whole cast is what I would say, perfect for this movie. DON\\'T MISS IT! YOU\\'LL REGRET IT!'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Combining the data sets and removing stop words \n"}, {"metadata": {}, "cell_type": "code", "source": "# As we can see the data has lot of stop words removing stop words before processing using keras\ndef filter_stop_words(train_sentences, stop_words):\n    for i, sentence in enumerate(train_sentences):\n        new_sent = [word for word in sentence.split() if word not in stop_words]\n        train_sentences[i] = ' '.join(new_sent)\n    return train_sentences\n\n", "execution_count": 76, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'train_sentences' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-76-da8710a13a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'train_sentences' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "stop_words = set(stopwords.words(\"english\"))\nx_train_text = filter_stop_words(x_train_text, stop_words)\nx_test_text = filter_stop_words(x_test_text, stop_words)\ndata_all = x_train_text + x_test_text", "execution_count": 77, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "num_words = 1000", "execution_count": 78, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Tokenise the train data set using Keras"}, {"metadata": {}, "cell_type": "code", "source": "from keras.models import Sequential\nfrom keras.layers import Dense, GRU, Embedding,LSTM\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences", "execution_count": 79, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.spatial.distance import cdist", "execution_count": 80, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Create a tokeniser using keras"}, {"metadata": {}, "cell_type": "markdown", "source": "Preparing the Embedding Layer\nAs a first step, we will use the Tokenizer class from the keras.preprocessing.text module to create a word-to-index dictionary. In the word-to-index dictionary, each word in the corpus is used as a key, while a corresponding unique index is used as the value for the key."}, {"metadata": {}, "cell_type": "code", "source": "#Create Vocab to Int mapping dictionary using Toeknizer from keras\n\ntokenizer = Tokenizer(num_words=num_words)", "execution_count": 81, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "## Fit the tokenizer on all the  data\n\ntokenizer.fit_on_texts(data_all)", "execution_count": 82, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get the word index from the train data \ntokenizer.word_index", "execution_count": 83, "outputs": [{"output_type": "execute_result", "execution_count": 83, "data": {"text/plain": "{'br': 1,\n 'i': 2,\n 'the': 3,\n 'movie': 4,\n 'film': 5,\n 'one': 6,\n 'it': 7,\n 'like': 8,\n 'this': 9,\n 'good': 10,\n 'time': 11,\n 'even': 12,\n 'would': 13,\n 'really': 14,\n 'see': 15,\n 'story': 16,\n 'and': 17,\n 'well': 18,\n 'much': 19,\n 'bad': 20,\n 'get': 21,\n 'great': 22,\n 'also': 23,\n 'people': 24,\n 'first': 25,\n 'but': 26,\n 'in': 27,\n 'made': 28,\n 'make': 29,\n 'a': 30,\n 'way': 31,\n 'movies': 32,\n 'could': 33,\n 'think': 34,\n 'characters': 35,\n 'watch': 36,\n 'films': 37,\n 'two': 38,\n 'many': 39,\n 'seen': 40,\n 'character': 41,\n 'there': 42,\n 'never': 43,\n 'plot': 44,\n 'love': 45,\n 'acting': 46,\n 'life': 47,\n 'best': 48,\n \"it's\": 49,\n 'know': 50,\n 'show': 51,\n 'little': 52,\n 'ever': 53,\n 'if': 54,\n 'all': 55,\n 'better': 56,\n 'end': 57,\n 'man': 58,\n 'scene': 59,\n 'still': 60,\n 'say': 61,\n 'scenes': 62,\n 'he': 63,\n 'that': 64,\n 'something': 65,\n 'go': 66,\n 'back': 67,\n 'you': 68,\n 'real': 69,\n \"i'm\": 70,\n 'watching': 71,\n 'thing': 72,\n 'actors': 73,\n 'though': 74,\n 'funny': 75,\n 'years': 76,\n 'as': 77,\n 'old': 78,\n '10': 79,\n 'another': 80,\n 'not': 81,\n 'work': 82,\n 'so': 83,\n 'actually': 84,\n 'nothing': 85,\n 'makes': 86,\n 'look': 87,\n 'director': 88,\n 'find': 89,\n 'going': 90,\n 'new': 91,\n 'lot': 92,\n 'is': 93,\n 'every': 94,\n 'part': 95,\n 'cast': 96,\n 'us': 97,\n 'things': 98,\n 'want': 99,\n 'quite': 100,\n 'what': 101,\n 'pretty': 102,\n 'world': 103,\n 'horror': 104,\n 'around': 105,\n 'seems': 106,\n \"can't\": 107,\n 'young': 108,\n 'take': 109,\n 'however': 110,\n 'got': 111,\n 'thought': 112,\n 'big': 113,\n 'fact': 114,\n 'enough': 115,\n 'long': 116,\n 'on': 117,\n 'they': 118,\n \"that's\": 119,\n 'give': 120,\n \"i've\": 121,\n 'may': 122,\n 'for': 123,\n 'comedy': 124,\n 'right': 125,\n 'series': 126,\n 'action': 127,\n 'must': 128,\n 'music': 129,\n 'without': 130,\n 'times': 131,\n 'saw': 132,\n 'always': 133,\n 'original': 134,\n 'role': 135,\n 'come': 136,\n 'almost': 137,\n 'me': 138,\n 'to': 139,\n 'gets': 140,\n 'interesting': 141,\n 'guy': 142,\n 'point': 143,\n 'done': 144,\n \"there's\": 145,\n 'whole': 146,\n 'least': 147,\n 'far': 148,\n 'bit': 149,\n 'script': 150,\n 'minutes': 151,\n 'feel': 152,\n '2': 153,\n 'of': 154,\n 'making': 155,\n 'anything': 156,\n 'might': 157,\n 'since': 158,\n 'family': 159,\n \"he's\": 160,\n 'last': 161,\n 'probably': 162,\n 'tv': 163,\n 'performance': 164,\n 'kind': 165,\n 'away': 166,\n 'out': 167,\n 'yet': 168,\n 'fun': 169,\n 'here': 170,\n 'worst': 171,\n 'no': 172,\n 'when': 173,\n 'sure': 174,\n 'rather': 175,\n 'hard': 176,\n 'anyone': 177,\n 'girl': 178,\n 'she': 179,\n 'played': 180,\n 'day': 181,\n 'found': 182,\n 'up': 183,\n 'them': 184,\n 'him': 185,\n 'looking': 186,\n 'woman': 187,\n 'screen': 188,\n 'although': 189,\n 'especially': 190,\n 'believe': 191,\n 'trying': 192,\n 'course': 193,\n 'her': 194,\n 'dvd': 195,\n 'everything': 196,\n 'set': 197,\n 'with': 198,\n 'goes': 199,\n 'comes': 200,\n 'put': 201,\n 'ending': 202,\n 'maybe': 203,\n 'place': 204,\n 'book': 205,\n 'shows': 206,\n 'three': 207,\n 'worth': 208,\n 'different': 209,\n 'main': 210,\n 'sense': 211,\n 'now': 212,\n 'american': 213,\n 'reason': 214,\n 'my': 215,\n 'again': 216,\n 'looks': 217,\n 'effects': 218,\n 'watched': 219,\n 'true': 220,\n 'play': 221,\n 'money': 222,\n 'actor': 223,\n 'job': 224,\n 'together': 225,\n 'war': 226,\n 'someone': 227,\n 'at': 228,\n 'plays': 229,\n 'instead': 230,\n 'high': 231,\n 'year': 232,\n 'said': 233,\n 'half': 234,\n 'everyone': 235,\n 'later': 236,\n 'takes': 237,\n '1': 238,\n 'seem': 239,\n 'audience': 240,\n 'special': 241,\n 'beautiful': 242,\n 'left': 243,\n 'seeing': 244,\n 'then': 245,\n 'john': 246,\n 'night': 247,\n 'black': 248,\n 'version': 249,\n 'shot': 250,\n 'excellent': 251,\n 'we': 252,\n 'idea': 253,\n 'too': 254,\n 'house': 255,\n 'mind': 256,\n 'star': 257,\n 'wife': 258,\n 'fan': 259,\n 'death': 260,\n 'used': 261,\n 'else': 262,\n 'simply': 263,\n 'nice': 264,\n 'budget': 265,\n 'poor': 266,\n 'completely': 267,\n 'short': 268,\n 'second': 269,\n '3': 270,\n 'read': 271,\n 'less': 272,\n 'along': 273,\n 'why': 274,\n 'after': 275,\n 'top': 276,\n 'help': 277,\n 'home': 278,\n 'men': 279,\n 'either': 280,\n 'line': 281,\n 'boring': 282,\n 'dead': 283,\n 'friends': 284,\n 'kids': 285,\n 'try': 286,\n 'production': 287,\n 'enjoy': 288,\n 'wrong': 289,\n 'camera': 290,\n 'use': 291,\n 'given': 292,\n 'low': 293,\n 'father': 294,\n 'classic': 295,\n 'need': 296,\n 'full': 297,\n 'or': 298,\n 'stupid': 299,\n 'next': 300,\n 'performances': 301,\n 'school': 302,\n 'hollywood': 303,\n 'rest': 304,\n 'truly': 305,\n 'awful': 306,\n 'video': 307,\n 'who': 308,\n 'couple': 309,\n 'do': 310,\n 'start': 311,\n 'sex': 312,\n 'recommend': 313,\n 'women': 314,\n 'let': 315,\n 'while': 316,\n 'tell': 317,\n 'his': 318,\n 'terrible': 319,\n 'remember': 320,\n 'mean': 321,\n 'came': 322,\n 'understand': 323,\n 'getting': 324,\n 'perhaps': 325,\n 'moments': 326,\n 'name': 327,\n 'keep': 328,\n 'face': 329,\n 'wonderful': 330,\n 'playing': 331,\n 'human': 332,\n 'style': 333,\n 'small': 334,\n 'episode': 335,\n 'perfect': 336,\n 'others': 337,\n 'person': 338,\n 'often': 339,\n 'early': 340,\n 'stars': 341,\n 'definitely': 342,\n 'written': 343,\n 'head': 344,\n 'lines': 345,\n 'dialogue': 346,\n 'some': 347,\n 'gives': 348,\n 'off': 349,\n 'piece': 350,\n 'went': 351,\n 'finally': 352,\n 'mother': 353,\n 'title': 354,\n 'case': 355,\n 'absolutely': 356,\n 'yes': 357,\n 'boy': 358,\n 'live': 359,\n 'laugh': 360,\n 'certainly': 361,\n 'liked': 362,\n 'become': 363,\n 'oh': 364,\n 'worse': 365,\n 'entertaining': 366,\n 'sort': 367,\n 'was': 368,\n 'loved': 369,\n 'lost': 370,\n 'called': 371,\n 'hope': 372,\n 'picture': 373,\n \"don't\": 374,\n 'felt': 375,\n 'overall': 376,\n 'entire': 377,\n 'mr': 378,\n 'several': 379,\n 'based': 380,\n 'cinema': 381,\n 'supposed': 382,\n 'how': 383,\n 'friend': 384,\n 'guys': 385,\n 'sound': 386,\n 'be': 387,\n 'just': 388,\n '5': 389,\n 'problem': 390,\n 'drama': 391,\n 'over': 392,\n 'waste': 393,\n 'white': 394,\n 'beginning': 395,\n '4': 396,\n 'fans': 397,\n 'totally': 398,\n 'dark': 399,\n 'care': 400,\n 'direction': 401,\n 'humor': 402,\n 'wanted': 403,\n 'seemed': 404,\n 'game': 405,\n 'children': 406,\n 'despite': 407,\n 'which': 408,\n 'lives': 409,\n 'lead': 410,\n 'guess': 411,\n 'example': 412,\n 'already': 413,\n 'final': 414,\n 'turn': 415,\n 'evil': 416,\n 'throughout': 417,\n 'becomes': 418,\n 'unfortunately': 419,\n 'able': 420,\n 'from': 421,\n 'quality': 422,\n 'more': 423,\n \"i'd\": 424,\n 'days': 425,\n 'history': 426,\n 'fine': 427,\n 'side': 428,\n 'wants': 429,\n 'heart': 430,\n 'horrible': 431,\n 'writing': 432,\n 'amazing': 433,\n 'b': 434,\n 'flick': 435,\n 'killer': 436,\n 'run': 437,\n 'son': 438,\n '\\x96': 439,\n 'michael': 440,\n 'works': 441,\n 'close': 442,\n \"they're\": 443,\n 'act': 444,\n 'kill': 445,\n 'art': 446,\n 'matter': 447,\n 'etc': 448,\n 'tries': 449,\n 'past': 450,\n 'town': 451,\n 'enjoyed': 452,\n 'turns': 453,\n 'brilliant': 454,\n 'gave': 455,\n 'behind': 456,\n 'parts': 457,\n 'stuff': 458,\n 'genre': 459,\n 'eyes': 460,\n 'car': 461,\n 'favorite': 462,\n 'directed': 463,\n 'late': 464,\n 'hand': 465,\n 'expect': 466,\n 'other': 467,\n 'soon': 468,\n 'hour': 469,\n 'obviously': 470,\n 'sometimes': 471,\n 'killed': 472,\n 'thinking': 473,\n 'actress': 474,\n 'child': 475,\n 'girls': 476,\n 'viewer': 477,\n 'starts': 478,\n 'city': 479,\n 'decent': 480,\n 'highly': 481,\n 'type': 482,\n 'stop': 483,\n 'self': 484,\n 'god': 485,\n 'says': 486,\n 'group': 487,\n 'anyway': 488,\n 'voice': 489,\n 'took': 490,\n 'known': 491,\n 'blood': 492,\n 'kid': 493,\n 'heard': 494,\n 'happens': 495,\n 'except': 496,\n 'before': 497,\n 'fight': 498,\n 'feeling': 499,\n 'experience': 500,\n 'coming': 501,\n 'slow': 502,\n 'by': 503,\n 'daughter': 504,\n 'writer': 505,\n 'stories': 506,\n 'moment': 507,\n 'did': 508,\n 'told': 509,\n 'leave': 510,\n 'extremely': 511,\n 'score': 512,\n 'violence': 513,\n 'involved': 514,\n 'police': 515,\n 'about': 516,\n 'strong': 517,\n 'chance': 518,\n 'lack': 519,\n 'cannot': 520,\n 'hit': 521,\n 'hilarious': 522,\n 'roles': 523,\n 'wonder': 524,\n 'happen': 525,\n 'particularly': 526,\n 'ok': 527,\n 'including': 528,\n 'living': 529,\n 'save': 530,\n 'looked': 531,\n 'crap': 532,\n 'simple': 533,\n 'please': 534,\n 'cool': 535,\n 'murder': 536,\n 'obvious': 537,\n 'complete': 538,\n 'happened': 539,\n 'cut': 540,\n 'age': 541,\n 'serious': 542,\n 'gore': 543,\n 'attempt': 544,\n 'hell': 545,\n 'ago': 546,\n 'song': 547,\n 's': 548,\n 'shown': 549,\n 'taken': 550,\n 'english': 551,\n 'james': 552,\n 'robert': 553,\n 'seriously': 554,\n 'david': 555,\n 'released': 556,\n 'reality': 557,\n 'opening': 558,\n 'interest': 559,\n 'jokes': 560,\n 'across': 561,\n 'none': 562,\n 'hero': 563,\n 'today': 564,\n 'alone': 565,\n 'exactly': 566,\n 'possible': 567,\n 'sad': 568,\n 'brother': 569,\n 'career': 570,\n 'saying': 571,\n 'number': 572,\n \"film's\": 573,\n 'hours': 574,\n 'usually': 575,\n 'cinematography': 576,\n 'talent': 577,\n 'view': 578,\n 'running': 579,\n 'annoying': 580,\n 'relationship': 581,\n 'very': 582,\n 'documentary': 583,\n 'wish': 584,\n 'huge': 585,\n 'order': 586,\n 'whose': 587,\n 'shots': 588,\n 'ridiculous': 589,\n 'taking': 590,\n 'important': 591,\n 'body': 592,\n 'light': 593,\n 'middle': 594,\n 'level': 595,\n 'ends': 596,\n 'female': 597,\n 'call': 598,\n 'started': 599,\n 'husband': 600,\n \"i'll\": 601,\n 'four': 602,\n 'most': 603,\n 'power': 604,\n 'word': 605,\n 'turned': 606,\n 'major': 607,\n 'opinion': 608,\n 'an': 609,\n 'change': 610,\n 'mostly': 611,\n 'are': 612,\n 'usual': 613,\n 'scary': 614,\n 'silly': 615,\n 'rating': 616,\n 'beyond': 617,\n 'somewhat': 618,\n 'ones': 619,\n 'happy': 620,\n 'room': 621,\n 'words': 622,\n 'knows': 623,\n 'knew': 624,\n 'country': 625,\n 'disappointed': 626,\n 'talking': 627,\n 'novel': 628,\n 'apparently': 629,\n 'non': 630,\n 'strange': 631,\n 'attention': 632,\n 'upon': 633,\n 'finds': 634,\n 'single': 635,\n 'basically': 636,\n 'cheap': 637,\n 'modern': 638,\n 'due': 639,\n 'jack': 640,\n 'television': 641,\n 'musical': 642,\n 'down': 643,\n 'problems': 644,\n 'miss': 645,\n 'episodes': 646,\n 'clearly': 647,\n 'local': 648,\n '7': 649,\n 'british': 650,\n 'thriller': 651,\n 'talk': 652,\n 'events': 653,\n 'five': 654,\n 'sequence': 655,\n 'class': 656,\n 'french': 657,\n 'moving': 658,\n 'ten': 659,\n 'fast': 660,\n 'earth': 661,\n 'review': 662,\n 'tells': 663,\n 'predictable': 664,\n 'team': 665,\n 'songs': 666,\n 'comic': 667,\n 'whether': 668,\n 'straight': 669,\n '8': 670,\n 'die': 671,\n 'add': 672,\n 'dialog': 673,\n 'entertainment': 674,\n 'future': 675,\n 'sets': 676,\n 'enjoyable': 677,\n 'appears': 678,\n 'near': 679,\n 'space': 680,\n 'easily': 681,\n 'hate': 682,\n 'soundtrack': 683,\n 'bring': 684,\n 'giving': 685,\n 'lots': 686,\n 'romantic': 687,\n 'similar': 688,\n 'george': 689,\n 'supporting': 690,\n 'release': 691,\n 'mention': 692,\n 'its': 693,\n 'within': 694,\n 'filmed': 695,\n 'message': 696,\n 'sequel': 697,\n 'clear': 698,\n 'needs': 699,\n 'falls': 700,\n 'dull': 701,\n 'suspense': 702,\n 'bunch': 703,\n 'eye': 704,\n 'surprised': 705,\n 'these': 706,\n 'showing': 707,\n 'tried': 708,\n 'sorry': 709,\n 'certain': 710,\n 'working': 711,\n 'easy': 712,\n 'ways': 713,\n 'theme': 714,\n 'theater': 715,\n 'among': 716,\n 'named': 717,\n \"what's\": 718,\n 'storyline': 719,\n 'monster': 720,\n 'king': 721,\n 'stay': 722,\n 'effort': 723,\n 'fall': 724,\n 'stand': 725,\n 'minute': 726,\n 'gone': 727,\n 'rock': 728,\n \"'\": 729,\n 'using': 730,\n '9': 731,\n 'only': 732,\n 'feature': 733,\n 'buy': 734,\n 'comments': 735,\n 'typical': 736,\n 'editing': 737,\n 'sister': 738,\n 'tale': 739,\n 'avoid': 740,\n 'mystery': 741,\n 'deal': 742,\n 'dr': 743,\n 'doubt': 744,\n 'fantastic': 745,\n 'okay': 746,\n 'nearly': 747,\n 'kept': 748,\n 'once': 749,\n 'feels': 750,\n 'subject': 751,\n 'viewing': 752,\n 'elements': 753,\n 'check': 754,\n 'oscar': 755,\n 'points': 756,\n 'realistic': 757,\n 'greatest': 758,\n 'means': 759,\n 't': 760,\n 'parents': 761,\n 'famous': 762,\n 'imagine': 763,\n 'rent': 764,\n 'viewers': 765,\n 'richard': 766,\n 'crime': 767,\n 'form': 768,\n 'peter': 769,\n 'actual': 770,\n 'general': 771,\n 'lady': 772,\n 'dog': 773,\n 'follow': 774,\n 'believable': 775,\n 'period': 776,\n 'red': 777,\n 'move': 778,\n 'material': 779,\n 'brought': 780,\n 'forget': 781,\n 'somehow': 782,\n 'begins': 783,\n 'reviews': 784,\n 'animation': 785,\n 'paul': 786,\n 'leads': 787,\n 'weak': 788,\n 'figure': 789,\n 'surprise': 790,\n 'sit': 791,\n 'hear': 792,\n 'average': 793,\n 'open': 794,\n 'sequences': 795,\n 'atmosphere': 796,\n 'killing': 797,\n 'eventually': 798,\n 'tom': 799,\n 'learn': 800,\n 'wait': 801,\n 'premise': 802,\n '20': 803,\n 'sci': 804,\n 'deep': 805,\n 'fi': 806,\n 'expected': 807,\n 'whatever': 808,\n 'indeed': 809,\n 'particular': 810,\n 'lame': 811,\n 'note': 812,\n 'poorly': 813,\n 'dance': 814,\n 'imdb': 815,\n 're': 816,\n 'shame': 817,\n 'situation': 818,\n 'third': 819,\n 'york': 820,\n 'box': 821,\n 'truth': 822,\n 'decided': 823,\n 'free': 824,\n 'hot': 825,\n \"who's\": 826,\n 'difficult': 827,\n 'needed': 828,\n 'season': 829,\n 'acted': 830,\n 'leaves': 831,\n 'unless': 832,\n 'romance': 833,\n 'possibly': 834,\n 'emotional': 835,\n 'gay': 836,\n 'sexual': 837,\n 'because': 838,\n 'boys': 839,\n 'footage': 840,\n 'write': 841,\n 'western': 842,\n 'forced': 843,\n 'credits': 844,\n 'doctor': 845,\n 'memorable': 846,\n 'became': 847,\n 'reading': 848,\n 'otherwise': 849,\n 'begin': 850,\n 'crew': 851,\n 'de': 852,\n 'air': 853,\n 'question': 854,\n 'society': 855,\n 'meet': 856,\n 'male': 857,\n 'meets': 858,\n \"let's\": 859,\n 'plus': 860,\n 'hands': 861,\n 'cheesy': 862,\n 'superb': 863,\n 'screenplay': 864,\n 'beauty': 865,\n 'interested': 866,\n 'street': 867,\n 'features': 868,\n 'masterpiece': 869,\n 'perfectly': 870,\n 'laughs': 871,\n 'nature': 872,\n 'stage': 873,\n 'effect': 874,\n 'forward': 875,\n 'comment': 876,\n 'badly': 877,\n 'sounds': 878,\n 'previous': 879,\n 'e': 880,\n 'japanese': 881,\n 'weird': 882,\n 'island': 883,\n 'personal': 884,\n 'inside': 885,\n 'quickly': 886,\n 'total': 887,\n 'himself': 888,\n 'keeps': 889,\n 'result': 890,\n 'towards': 891,\n 'america': 892,\n 'crazy': 893,\n 'battle': 894,\n 'worked': 895,\n 'incredibly': 896,\n 'setting': 897,\n 'earlier': 898,\n 'through': 899,\n 'background': 900,\n 'mess': 901,\n 'cop': 902,\n 'writers': 903,\n 'fire': 904,\n 'copy': 905,\n 'dumb': 906,\n 'unique': 907,\n 'realize': 908,\n 'lee': 909,\n 'powerful': 910,\n 'mark': 911,\n 'business': 912,\n 'rate': 913,\n 'older': 914,\n 'dramatic': 915,\n 'pay': 916,\n 'does': 917,\n 'following': 918,\n 'both': 919,\n 'girlfriend': 920,\n 'directors': 921,\n 'joke': 922,\n 'plenty': 923,\n 'directing': 924,\n 'various': 925,\n 'baby': 926,\n 'creepy': 927,\n 'appear': 928,\n 'development': 929,\n 'brings': 930,\n 'dream': 931,\n 'front': 932,\n 'ask': 933,\n 'water': 934,\n 'bill': 935,\n 'rich': 936,\n 'admit': 937,\n 'where': 938,\n 'apart': 939,\n 'joe': 940,\n 'fairly': 941,\n 'political': 942,\n 'reasons': 943,\n 'leading': 944,\n 'portrayed': 945,\n 'spent': 946,\n 'telling': 947,\n 'have': 948,\n 'cover': 949,\n 'outside': 950,\n 'fighting': 951,\n 'present': 952,\n 'wasted': 953,\n 'deserves': 954,\n 'fails': 955,\n 'era': 956,\n 'gun': 957,\n 'success': 958,\n 'break': 959,\n 'party': 960,\n 'meant': 961,\n 'list': 962,\n 'expecting': 963,\n 'william': 964,\n 'create': 965,\n 'attempts': 966,\n 'will': 967,\n 'ideas': 968,\n 'ended': 969,\n \"'the\": 970,\n 'manages': 971,\n 'recently': 972,\n 'secret': 973,\n 'agree': 974,\n 'missing': 975,\n 'talented': 976,\n 'twist': 977,\n 'zombie': 978,\n 'match': 979,\n 'caught': 980,\n 'itself': 981,\n 'fantasy': 982,\n 'unlike': 983,\n 'hardly': 984,\n 'cute': 985,\n 'odd': 986,\n 'large': 987,\n 'german': 988,\n 'produced': 989,\n 'return': 990,\n 'spoilers': 991,\n 'clever': 992,\n 'laughing': 993,\n 'co': 994,\n 'nudity': 995,\n 'plain': 996,\n 'remake': 997,\n 'disney': 998,\n 'members': 999,\n 'escape': 1000,\n ...}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Getting the tokens from Train Data"}, {"metadata": {}, "cell_type": "code", "source": "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)", "execution_count": 84, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "np.array(x_train_tokens[1])", "execution_count": 85, "outputs": [{"output_type": "execute_result", "execution_count": 85, "data": {"text/plain": "array([  2, 670,   4, 467,  64,   4,   6,  49, 366,  61, 147,   3,  62,\n       589,  74,   2,  34, 308, 205, 237,  52, 149,  19,   7,   7, 106,\n         8,  42,  21, 250, 228, 110,   4, 133,  86,   7,  92,  22, 402,\n        86, 360,  18, 863, 223,   2, 128,  61,  63, 206,  19,   9,  25,\n        11,  46, 746,   3, 135, 185, 909, 356,  22,   3, 146,  96,   2,\n        13,  61, 336,   4, 374, 645,   7,   7])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)", "execution_count": 86, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We set the maximum size of each list to 250. The lists with size greater than 250 will be truncated to 250.This process is called padding."}, {"metadata": {}, "cell_type": "code", "source": "max_len = 250\nX_train = pad_sequences(x_train_tokens, maxlen=max_len)", "execution_count": 89, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_test = pad_sequences(x_test_tokens, maxlen=max_len)", "execution_count": 90, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train.shape", "execution_count": 93, "outputs": [{"output_type": "execute_result", "execution_count": 93, "data": {"text/plain": "(25000, 250)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Creating an LTSM model using Keras"}, {"metadata": {}, "cell_type": "code", "source": "model = Sequential()\nembedding_size = 8\nmodel.add(Embedding(input_dim=num_words,\n                    output_dim=embedding_size,\n                    input_length=max_len,\n                    name='layer_embedding'))", "execution_count": 103, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model.add(LSTM(units=16, return_sequences=True))\nmodel.add(LSTM(units=8, return_sequences=True))\nmodel.add(LSTM(units=4))\nmodel.add(Dense(1))", "execution_count": 104, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "optimizer = Adam(lr=1e-3)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n", "execution_count": 105, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "To compile our model, we will use the adam optimizer, binary_crossentropy as our loss function and accuracy as metrics and then we will print the summary of our model:"}, {"metadata": {}, "cell_type": "code", "source": "model.summary()", "execution_count": 106, "outputs": [{"output_type": "stream", "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlayer_embedding (Embedding)  (None, 250, 8)            8000      \n_________________________________________________________________\nlstm_6 (LSTM)                (None, 250, 16)           1600      \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 250, 8)            800       \n_________________________________________________________________\nlstm_8 (LSTM)                (None, 4)                 208       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 5         \n=================================================================\nTotal params: 10,613\nTrainable params: 10,613\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The model is configured to train on 23750 samples and validate on 1250 samples"}, {"metadata": {}, "cell_type": "code", "source": "history = model.fit(X_train, y_train,\n          validation_split=0.05, epochs=1, batch_size=64)", "execution_count": 112, "outputs": [{"output_type": "stream", "text": "Train on 23750 samples, validate on 1250 samples\nEpoch 1/1\n23750/23750 [==============================] - 1124s 47ms/step - loss: 0.3889 - acc: 0.8329 - val_loss: 0.5020 - val_acc: 0.7376\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "To evaluate the performance of the model, we can simply pass the test set to the evaluate method of our model."}, {"metadata": {}, "cell_type": "code", "source": "result = model.evaluate(X_test, y_test)\nprint(\"Accuracy: {0:.2%}\".format(result[1]))", "execution_count": 113, "outputs": [{"output_type": "stream", "text": "25000/25000 [==============================] - 44s 2ms/step\nAccuracy: 82.65%\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "To check the test accuracy and loss, execute the following script:"}, {"metadata": {}, "cell_type": "code", "source": "print(\"Test Score:\", result[0])\nprint(\"Test Accuracy:\", result[1])", "execution_count": 114, "outputs": [{"output_type": "stream", "text": "Test Score: 0.37567781929016114\nTest Accuracy: 0.82652\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "We get a test accuracy of 82.652%. Our training accuracy was 83.29%. This means that our model is correctly on the training set. \nThe performance difference between training and test sets should be minimum.\n"}, {"metadata": {}, "cell_type": "code", "source": "history.history", "execution_count": 119, "outputs": [{"output_type": "execute_result", "execution_count": 119, "data": {"text/plain": "{'val_loss': [0.50202592086792],\n 'val_acc': [0.7376000002861023],\n 'loss': [0.38887465180848774],\n 'acc': [0.8328842105313351]}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Making Predictions on Single Instance"}, {"metadata": {}, "cell_type": "markdown", "source": "Let us test the predictions aganist 5000 rows from test data "}, {"metadata": {}, "cell_type": "markdown", "source": "Prediction for Test Data"}, {"metadata": {}, "cell_type": "code", "source": "y_pred = model.predict(x=X_test[0:5000])\ny_pred = y_pred.T[0]", "execution_count": 138, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "These predicted numbers would be between 0.0 and 1.0. So we set a threshold value of 0.5,and say that all values above 0.5 are taken to be 1.0 and all values below 0.5 are taken to be 0.0. This gives us predicted \"class\" of either 0.0 or 1.0."}, {"metadata": {}, "cell_type": "code", "source": "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])", "execution_count": 139, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cls_true = np.array(y_test[0:5000])", "execution_count": 140, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "incorrect = np.where(cls_pred != cls_true)\nincorrect = incorrect[0]", "execution_count": 141, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(incorrect)", "execution_count": 142, "outputs": [{"output_type": "execute_result", "execution_count": 142, "data": {"text/plain": "334"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "The Model predicted 334 rows incorrectly"}, {"metadata": {}, "cell_type": "code", "source": "idx = incorrect[0]\nidx", "execution_count": 147, "outputs": [{"output_type": "execute_result", "execution_count": 147, "data": {"text/plain": "17"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Sample mis-classified text is:"}, {"metadata": {}, "cell_type": "code", "source": "text = x_test_text[idx]\ntext", "execution_count": 148, "outputs": [{"output_type": "execute_result", "execution_count": 148, "data": {"text/plain": "\"seriously people need lighten accept funny funny, movie f**king hilarious. Better first Knoxville really grew pair film way crazier stunts first. If Ebert Roper(not saying I'm huge fan theirs) look past pure idiocy film enough give 2 thumbs think people to. I sure expect floored rare sequel better original. This new one I believe exceeds first big time. did,just relax kick back try barf points laugh ass off.\""}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "The predicted value of sentiment is"}, {"metadata": {}, "cell_type": "code", "source": "y_pred[idx]", "execution_count": 151, "outputs": [{"output_type": "execute_result", "execution_count": 151, "data": {"text/plain": "0.28527364"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "cls_true[idx]", "execution_count": 150, "outputs": [{"output_type": "execute_result", "execution_count": 150, "data": {"text/plain": "1.0"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}